{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# MODELO FINAL SEPARA EN 6 COLUMNAS (FECHA, HORA, TITULO DE NOTICIA, AUTOR, TEMA, RESUMEN)\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "\n",
        "# Montamos Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define la URL base y las fechas de inicio y fin\n",
        "url_base = 'https://elcomercio.pe/archivo/todas/'\n",
        "fecha_inicio = '2023-08-31' # '2022-09-01'\n",
        "fecha_fin = '2023-09-30' # '2023-09-30'\n",
        "\n",
        "# Lista para almacenar los DataFrames de cada fecha\n",
        "dfs = []\n",
        "\n",
        "# Recorre las fechas desde fecha_inicio hasta fecha_fin\n",
        "current_fecha = fecha_inicio\n",
        "while current_fecha <= fecha_fin:\n",
        "    # Construye la URL con la fecha actual\n",
        "    url = f'{url_base}{current_fecha}/'\n",
        "\n",
        "    # Realizamos una solicitud GET a la página web\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Verificamos si la solicitud fue exitosa\n",
        "    if response.status_code == 200:\n",
        "        content = response.text\n",
        "\n",
        "        # Parsear el contenido HTML utilizando BeautifulSoup\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        # Encontrar todo el texto relevante\n",
        "        fecha_elements = soup.find_all('span', class_='story-item__date-time')\n",
        "        hora_elements = soup.find_all('span', class_='story-item__date-time')\n",
        "        titulo_elements = soup.find_all('h2', class_='story-item__content-title')\n",
        "        autor_elements = soup.find_all('a', class_='story-item__author block uppercase mt-10 font-thin text-xs text-gray-200')\n",
        "        tema_elements = soup.find_all('a', class_='story-item__section story-item__section--desktop text-sm text-black md:mb-15 hidden')\n",
        "        resumen_elements = soup.find_all('p', class_='story-item__subtitle')\n",
        "\n",
        "        # Extraer el texto de los elementos encontrados\n",
        "        fechas = [fecha.text for fecha in fecha_elements[::2]]\n",
        "        horas = [hora.text for hora in hora_elements[1::2]]\n",
        "        titulos = [titulo.find('a').text for titulo in titulo_elements if titulo.find('a')]\n",
        "        autores = [autor.text for autor in autor_elements]\n",
        "        temas = [tema.text for tema in tema_elements]\n",
        "        resumenes = [resumen.text for resumen in resumen_elements]\n",
        "\n",
        "        # Asegurarse de que ambas listas tengan la misma longitud\n",
        "        min_length = min(len(fechas), len(horas), len(titulos), len(autores))\n",
        "\n",
        "        fechas = fechas[:min_length]\n",
        "        horas = horas[:min_length]\n",
        "        titulos = titulos[:min_length]\n",
        "        autores = autores[:min_length]\n",
        "\n",
        "        # Crear un DataFrame de pandas con el texto en columnas\n",
        "        df = pd.DataFrame({'Fecha': fechas, 'Hora': horas, 'Titulo de noticia': titulos, 'Autor': autores, 'Tema': temas, 'Resumen': resumenes})\n",
        "\n",
        "        # Agregar el DataFrame a la lista\n",
        "        dfs.append(df)\n",
        "    else:\n",
        "        print(f\"No se pudo acceder a la página web para la fecha {current_fecha}.\")\n",
        "\n",
        "    # Avanzar a la siguiente fecha\n",
        "    current_fecha = pd.to_datetime(current_fecha) + pd.DateOffset(days=1)\n",
        "    current_fecha = current_fecha.strftime('%Y-%m-%d')\n",
        "\n",
        "# Concatenar todos los DataFrames en uno solo\n",
        "final_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Especifica la ruta completa del archivo CSV en tu Google Drive\n",
        "csv_filepath = '/content/drive/My Drive/scrapy/ScrapingTexto.csv'\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV en Google Drive\n",
        "final_df.to_csv(csv_filepath, index=False, escapechar='\\\\')\n",
        "\n",
        "print(f\"Se ha extraído y guardado el texto en '{csv_filepath}'.\")\n",
        "print(len(final_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alUo9f9yq4mW",
        "outputId": "2a50b864-9409-4e0e-ea18-081dffb747d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Se ha extraído y guardado el texto en '/content/drive/My Drive/scrapy/ScrapingTexto.csv'.\n",
            "3100\n"
          ]
        }
      ]
    }
  ]
}